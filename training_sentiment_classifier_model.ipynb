{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Sentiment Classifier Model from a pre-trained Transformer Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install necessary libraries via command line\n",
    "```\n",
    "pip install spacy\n",
    "pip install spacy-transformers\n",
    "python -m spacy download en_trf_bertbaseuncased_lg\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import pre-trained language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.util import minibatch\n",
    "import en_trf_bertbaseuncased_lg\n",
    "import random\n",
    "import torch\n",
    "    \n",
    "nlp = spacy.load(\"en_trf_bertbaseuncased_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8573929\n",
      "0.7108537\n"
     ]
    }
   ],
   "source": [
    "# Explore text similarity feature of pre-trained language model\n",
    "\n",
    "text_1 = nlp(\"Dangerous virus has been spreading across the world amid health concerns.\")\n",
    "text_2 = nlp(\"Health authoirities warned about the virus in January.\")\n",
    "text_3 = nlp(\"Computer virus can be bad for economy as hacking cases increases across the world.\")\n",
    "print(text_1[1].similarity(text_2[5]))\n",
    "print(text_1[1].similarity(text_3[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1358, 3)\n",
      "neg    0.480663\n",
      "neu    0.281768\n",
      "pos    0.237569\n",
      "Name: label, dtype: float64\n",
      "neg    0.481618\n",
      "neu    0.279412\n",
      "pos    0.238971\n",
      "Name: label, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from helper_functions import preprocess, remove_emojis\n",
    "\n",
    "df = pd.read_csv('data/tweets/tweets_labeled.csv', encoding='utf-8', )\n",
    "df = df[['id', 'label', 'text']]\n",
    "df = df[df['label'].notnull()]\n",
    "print(df.shape)\n",
    "df['text'] = df['text'].replace(r'\\n',' ',regex=True)\n",
    "df['text'] = df['text'].apply(preprocess)\n",
    "df['text'] = df['text'].str.lower()\n",
    "\n",
    "df['label'] = df['label'].astype(int).astype(str)\n",
    "\n",
    "df['label'] = df['label'].map({'-1': 'neg', '0': 'neu', '1': 'pos'})\n",
    "\n",
    "X = df[['text', 'id']]\n",
    "y = df[['label']]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                stratify=y, \n",
    "                                                test_size=0.20)\n",
    "\n",
    "df_validation = pd.concat([X_test, y_test], axis=1)[['id', 'label', 'text']].reset_index(drop=True)\n",
    "df_train = pd.concat([X_train, y_train], axis=1)[['id', 'label', 'text']].reset_index(drop=True)\n",
    "\n",
    "print(df_train.label.value_counts(normalize=True))\n",
    "print(df_validation.label.value_counts(normalize=True))\n",
    "\n",
    "# df_train.to_csv('train.tsv', encoding='utf-8', index=False, sep='\\t')\n",
    "# df_validation.to_csv('test.tsv', encoding='utf-8', index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert train data and test data to a format where model expects\n",
    "records = pd.get_dummies(df_train[['label', 'text']], columns=['label'], dtype='float').to_dict(orient='records')\n",
    "TRAIN_DATA = []\n",
    "for record in records:\n",
    "    cats = record.copy()\n",
    "    cats.pop('text')\n",
    "    train_record = (record.get('text'), {\"cats\": cats})\n",
    "    TRAIN_DATA.append(train_record)\n",
    "    \n",
    "records = pd.get_dummies(df_validation[['label', 'text']], columns=['label'], dtype='float').to_dict(orient='records')\n",
    "EVAL_DATA = []\n",
    "for record in records:\n",
    "    cats = record.copy()\n",
    "    cats.pop('text')\n",
    "    eval_record = (record.get('text'), {\"cats\": cats})\n",
    "    EVAL_DATA.append(eval_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training sentiment classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import json\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import spacy\n",
    "import torch\n",
    "from spacy.util import minibatch\n",
    "import tqdm\n",
    "import unicodedata\n",
    "import wasabi\n",
    "from spacy_transformers.util import cyclic_triangular_rate\n",
    "import en_trf_bertbaseuncased_lg\n",
    "\n",
    "\n",
    "def read_inputs(train_data):\n",
    "    texts = []\n",
    "    cats = []\n",
    "    for line in train_data:\n",
    "        text, gold = line\n",
    "        text = preprocess_text(text)\n",
    "        texts.append(text)\n",
    "        cats.append(gold[\"cats\"])\n",
    "    return texts, cats\n",
    "\n",
    "\n",
    "def make_sentence_examples(nlp, texts, labels):\n",
    "    \"\"\"Treat each sentence of the document as an instance, using the doc labels.\"\"\"\n",
    "    sents = []\n",
    "    sent_cats = []\n",
    "    for text, cats in zip(texts, labels):\n",
    "        doc = nlp.make_doc(text)\n",
    "        doc = nlp.get_pipe(\"sentencizer\")(doc)\n",
    "        for sent in doc.sents:\n",
    "            sents.append(sent.text)\n",
    "            sent_cats.append(cats)\n",
    "    return sents, sent_cats\n",
    "\n",
    "\n",
    "white_re = re.compile(r\"\\s\\s+\")\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.replace(\"<s>\", \"<open-s-tag>\")\n",
    "    text = text.replace(\"</s>\", \"<close-s-tag>\")\n",
    "    text = white_re.sub(\" \", text).strip()\n",
    "    return \"\".join(\n",
    "        c for c in unicodedata.normalize(\"NFD\", text) if unicodedata.category(c) != \"Mn\"\n",
    "    )\n",
    "\n",
    "\n",
    "def evaluate(nlp, texts, cats, pos_label):\n",
    "    tp = 0.0  # True positives\n",
    "    fp = 0.0  # False positives\n",
    "    fn = 0.0  # False negatives\n",
    "    tn = 0.0  # True negatives\n",
    "    total_words = sum(len(text.split()) for text in texts)\n",
    "    with tqdm.tqdm(total=total_words, leave=False) as pbar:\n",
    "        for i, doc in enumerate(nlp.pipe(texts, batch_size=8)):\n",
    "            gold = cats[i]\n",
    "            for label, score in doc.cats.items():\n",
    "                if label not in gold:\n",
    "                    continue\n",
    "                if label != pos_label:\n",
    "                    continue\n",
    "                if score >= 0.5 and gold[label] >= 0.5:\n",
    "                    tp += 1.0\n",
    "                elif score >= 0.5 and gold[label] < 0.5:\n",
    "                    fp += 1.0\n",
    "                elif score < 0.5 and gold[label] < 0.5:\n",
    "                    tn += 1\n",
    "                elif score < 0.5 and gold[label] >= 0.5:\n",
    "                    fn += 1\n",
    "            pbar.update(len(doc.text.split()))\n",
    "    precision = tp / (tp + fp + 1e-8)\n",
    "    recall = tp / (tp + fn + 1e-8)\n",
    "    if (precision + recall) == 0:\n",
    "        f_score = 0.0\n",
    "    else:\n",
    "        f_score = 2 * (precision * recall) / (precision + recall)\n",
    "    return {\"textcat_p\": precision, \"textcat_r\": recall, \"textcat_f\": f_score}\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model=\"en_trf_bertbaseuncased_lg\",\n",
    "    train_data=None,\n",
    "    eval_data=None,\n",
    "    output_dir=None,\n",
    "    n_iter=5,\n",
    "    n_texts=100,\n",
    "    batch_size=8,\n",
    "    learn_rate=2e-5,\n",
    "    max_wpb=1000,\n",
    "    use_test=False,\n",
    "    pos_label=None,\n",
    "):\n",
    "    spacy.util.fix_random_seed(0)\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "\n",
    "    try:\n",
    "        nlp = spacy.load(model)\n",
    "    except:\n",
    "        nlp = en_trf_bertbaseuncased_lg.load()\n",
    "        \n",
    "    print(nlp.pipe_names)\n",
    "    print(f\"Loaded model '{model}'\")\n",
    "    textcat = nlp.create_pipe(\n",
    "        \"trf_textcat\",\n",
    "        config={\"architecture\": \"softmax_last_hidden\", \"words_per_batch\": max_wpb},\n",
    "    )\n",
    "    \n",
    "    train_texts, train_cats = read_inputs(train_data)\n",
    "    eval_texts, eval_cats = read_inputs(eval_data)\n",
    "    labels = set()\n",
    "    for cats in train_cats + eval_cats:\n",
    "        labels.update(cats)\n",
    "    # use the first label in the set as the positive label if one isn't\n",
    "    # provided\n",
    "    for label in sorted(labels):\n",
    "        if not pos_label:\n",
    "            pos_label = label\n",
    "        textcat.add_label(label)\n",
    "\n",
    "\n",
    "    print(\"Labels:\", textcat.labels)\n",
    "    print(\"Positive label for evaluation:\", pos_label)\n",
    "    nlp.add_pipe(textcat, last=True)\n",
    "    print(f\"Using {len(train_texts)} training docs, {len(eval_texts)} evaluation\")\n",
    "    split_training_by_sentence = False\n",
    "    if split_training_by_sentence:\n",
    "        # If we're using a model that averages over sentence predictions (we are),\n",
    "        # there are some advantages to just labelling each sentence as an example.\n",
    "        # It means we can mix the sentences into different batches, so we can make\n",
    "        # more frequent updates. It also changes the loss somewhat, in a way that's\n",
    "        # not obviously better -- but it does seem to work well.\n",
    "        train_texts, train_cats = make_sentence_examples(nlp, train_texts, train_cats)\n",
    "        print(f\"Extracted {len(train_texts)} training sents\")\n",
    "    # total_words = sum(len(text.split()) for text in train_texts)\n",
    "    train_data = list(zip(train_texts, [{\"cats\": cats} for cats in train_cats]))\n",
    "    # Initialize the TextCategorizer, and create an optimizer.\n",
    "    optimizer = nlp.resume_training()\n",
    "    optimizer.alpha = 0.001\n",
    "    optimizer.trf_weight_decay = 0.005\n",
    "    optimizer.L2 = 0.0\n",
    "    learn_rates = cyclic_triangular_rate(\n",
    "        learn_rate / 3, learn_rate * 3, 2 * len(train_data) // batch_size\n",
    "    )\n",
    "    print(\"Training the model...\")\n",
    "    print(\"{:^5}\\t{:^5}\\t{:^5}\\t{:^5}\".format(\"LOSS\", \"P\", \"R\", \"F\"))\n",
    "\n",
    "    pbar = tqdm.tqdm(total=100, leave=False)\n",
    "    results = []\n",
    "    epoch = 0\n",
    "    step = 0\n",
    "    eval_every = 100\n",
    "    patience = 3\n",
    "    while True:\n",
    "        # Train and evaluate\n",
    "        losses = Counter()\n",
    "        random.shuffle(train_data)\n",
    "        batches = minibatch(train_data, size=batch_size)\n",
    "        for batch in batches:\n",
    "            optimizer.trf_lr = next(learn_rates)\n",
    "            texts, annotations = zip(*batch)\n",
    "            nlp.update(texts, annotations, sgd=optimizer, drop=0.1, losses=losses)\n",
    "            pbar.update(1)\n",
    "            if step and (step % eval_every) == 0:\n",
    "                pbar.close()\n",
    "                with nlp.use_params(optimizer.averages):\n",
    "                    scores = evaluate(nlp, eval_texts, eval_cats, pos_label)\n",
    "                results.append((scores[\"textcat_f\"], step, epoch))\n",
    "                print(\n",
    "                    \"{0:.3f}\\t{1:.3f}\\t{2:.3f}\\t{3:.3f}\".format(\n",
    "                        losses[\"trf_textcat\"],\n",
    "                        scores[\"textcat_p\"],\n",
    "                        scores[\"textcat_r\"],\n",
    "                        scores[\"textcat_f\"],\n",
    "                    )\n",
    "                )\n",
    "                pbar = tqdm.tqdm(total=eval_every, leave=False)\n",
    "            step += 1\n",
    "        epoch += 1\n",
    "        # Stop if no improvement in HP.patience checkpoints\n",
    "        if results:\n",
    "            best_score, best_step, best_epoch = max(results)\n",
    "            if ((step - best_step) // eval_every) >= patience:\n",
    "                break\n",
    "\n",
    "    msg = wasabi.Printer()\n",
    "    table_widths = [2, 4, 6]\n",
    "    msg.info(f\"Best scoring checkpoints\")\n",
    "    msg.row([\"Epoch\", \"Step\", \"Score\"], widths=table_widths)\n",
    "    msg.row([\"-\" * width for width in table_widths])\n",
    "    for score, step, epoch in sorted(results, reverse=True)[:10]:\n",
    "        msg.row([epoch, step, \"%.2f\" % (score * 100)], widths=table_widths)\n",
    "\n",
    "    # Test the trained model\n",
    "    test_text = eval_texts[0]\n",
    "    doc = nlp(test_text)\n",
    "    print(test_text, doc.cats)\n",
    "\n",
    "    if output_dir is not None:\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)\n",
    "        # test the saved model\n",
    "        print(\"Loading from\", output_dir)\n",
    "        nlp2 = spacy.load(output_dir)\n",
    "        doc2 = nlp2(test_text)\n",
    "        print(test_text, doc2.cats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sentencizer', 'trf_wordpiecer', 'trf_tok2vec']\n",
      "Loaded model 'en_trf_bertbaseuncased_lg'\n",
      "Labels: ('label_neg', 'label_neu', 'label_pos')\n",
      "Positive label for evaluation: label_pos\n",
      "Using 1086 training docs, 272 evaluation\n",
      "Training the model...\n",
      "LOSS \t  P  \t  R  \t  F  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.726\t0.771\t0.831\t0.800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.177\t0.831\t0.831\t0.831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.051\t0.775\t0.846\t0.809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.221\t0.818\t0.831\t0.824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.019\t0.812\t0.862\t0.836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 15/100 [01:10<06:26,  4.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Best scoring checkpoints\u001b[0m\n",
      "Epoch   Step   Score \n",
      "--   ----   ------\n",
      "3    500    84.62 \n",
      "4    600    83.58 \n",
      "1    200    83.08 \n",
      "5    700    82.71 \n",
      "2    400    82.44 \n",
      "5    800    82.09 \n",
      "2    300    80.88 \n",
      "0    100    80.00 \n",
      "tributes paid to the muchloved amp compassionate bury gp dr saad aldubbaisi who has died with coronavirus the iraqiborn gp graduated from the university of baghdad amp had worked in the bury area for 20 years rest in peace {'label_neg': 2.5065755835385062e-05, 'label_neu': 3.924364864360541e-05, 'label_pos': 0.9999356269836426}\n",
      "Saved model to /home/jupyter/projects/sentiment-classifier\n",
      "Loading from /home/jupyter/projects/sentiment-classifier\n",
      "tributes paid to the muchloved amp compassionate bury gp dr saad aldubbaisi who has died with coronavirus the iraqiborn gp graduated from the university of baghdad amp had worked in the bury area for 20 years rest in peace {'label_neg': 2.5065755835385062e-05, 'label_neu': 3.924364864360541e-05, 'label_pos': 0.9999356269836426}\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"model\": \"en_trf_bertbaseuncased_lg\",\n",
    "    \"train_data\": TRAIN_DATA,\n",
    "    \"eval_data\": EVAL_DATA,\n",
    "    \"output_dir\": \"models/sentiment-classifier\",\n",
    "    \"use_test\": False,\n",
    "    \"batch_size\": 8,\n",
    "    \"learn_rate\": 2e-5,\n",
    "    \"max_wpb\": 1000,\n",
    "    \"n_texts\": 100,\n",
    "    \"n_iter\": 5,\n",
    "    \"pos_label\": \"label_pos\",\n",
    "}\n",
    "\n",
    "\n",
    "train_model(**params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import unicodedata\n",
    "\n",
    "# Review model output\n",
    "\n",
    "def read_inputs(train_data):\n",
    "    texts = []\n",
    "    cats = []\n",
    "    for line in train_data:\n",
    "        text, gold = line\n",
    "        text = preprocess_text(text)\n",
    "        texts.append(text)\n",
    "        cats.append(gold[\"cats\"])\n",
    "    return texts, cats\n",
    "\n",
    "\n",
    "white_re = re.compile(r\"\\s\\s+\")\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.replace(\"<s>\", \"<open-s-tag>\")\n",
    "    text = text.replace(\"</s>\", \"<close-s-tag>\")\n",
    "    text = white_re.sub(\" \", text).strip()\n",
    "    return \"\".join(\n",
    "        c for c in unicodedata.normalize(\"NFD\", text) if unicodedata.category(c) != \"Mn\"\n",
    "    )\n",
    "\n",
    "output_dir =  \"models/sentiment-classifier\"\n",
    "eval_texts, eval_cats = read_inputs(EVAL_DATA)\n",
    "i = 0 # Choose a random number to test\n",
    "test_text = eval_texts[i] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hundreds of ventilators the uk govt bought from china to relieve a major shortage during the covid19 pandemic are the wrong type and could kill patients senior doctors have warned {'label_neg': 0.9998445510864258, 'label_neu': 0.00012154843716416508, 'label_pos': 3.384325827937573e-05}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(output_dir)\n",
    "doc = nlp(test_text)\n",
    "print(test_text, doc.cats)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are some good news coming as coronavirus vaccine research advances. {'label_neg': 4.7637462557759136e-05, 'label_neu': 0.00017945301078725606, 'label_pos': 0.9997729659080505}\n"
     ]
    }
   ],
   "source": [
    "# Test the trained model with text of your choice\n",
    "test_text = \"There are some good news coming as coronavirus vaccine research advances.\"\n",
    "doc = nlp(test_text)\n",
    "print(test_text, doc.cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are some bad news coming as coronavirus vaccine research fails. {'label_neg': 0.9993046522140503, 'label_neu': 0.0006404068553820252, 'label_pos': 5.4913893109187484e-05}\n"
     ]
    }
   ],
   "source": [
    "# Test the trained model with text of your choice\n",
    "test_text = \"There are some bad news coming as coronavirus vaccine research fails.\"\n",
    "doc = nlp(test_text)\n",
    "print(test_text, doc.cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Copy of Finetune COVID-Twitter-BERT",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "name": "common-cu101.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu101:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
