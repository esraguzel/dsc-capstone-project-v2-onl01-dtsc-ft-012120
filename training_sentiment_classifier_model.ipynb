{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Sentiment Classifier Model from a pre-trained Transformer Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install necessary libraries via command line\n",
    "```\n",
    "pip install spacy\n",
    "pip install spacy-transformers\n",
    "python -m spacy download en_trf_bertbaseuncased_lg\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import pre-trained language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.util import minibatch\n",
    "import en_trf_bertbaseuncased_lg\n",
    "import random\n",
    "import torch\n",
    "    \n",
    "nlp = spacy.load(\"en_trf_bertbaseuncased_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8573929\n",
      "0.7108537\n"
     ]
    }
   ],
   "source": [
    "# Explore text similarity feature of pre-trained language model\n",
    "\n",
    "text_1 = nlp(\"Dangerous virus has been spreading across the world amid health concerns.\")\n",
    "text_2 = nlp(\"Health authoirities warned about the virus in January.\")\n",
    "text_3 = nlp(\"Computer virus can be bad for economy as hacking cases increases across the world.\")\n",
    "print(text_1[1].similarity(text_2[5]))\n",
    "print(text_1[1].similarity(text_3[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1358, 3)\n",
      "neg    0.480663\n",
      "neu    0.281768\n",
      "pos    0.237569\n",
      "Name: label, dtype: float64\n",
      "neg    0.481618\n",
      "neu    0.279412\n",
      "pos    0.238971\n",
      "Name: label, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from helper_functions import preprocess, remove_emojis\n",
    "\n",
    "df = pd.read_csv('data/tweets/tweets_labeled.csv', encoding='utf-8', )\n",
    "df = df[['id', 'label', 'text']]\n",
    "df = df[df['label'].notnull()]\n",
    "print(df.shape)\n",
    "df['text'] = df['text'].replace(r'\\n',' ',regex=True)\n",
    "df['text'] = df['text'].apply(preprocess)\n",
    "df['text'] = df['text'].str.lower()\n",
    "\n",
    "df['label'] = df['label'].astype(int).astype(str)\n",
    "\n",
    "df['label'] = df['label'].map({'-1': 'neg', '0': 'neu', '1': 'pos'})\n",
    "\n",
    "X = df[['text', 'id']]\n",
    "y = df[['label']]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                stratify=y, \n",
    "                                                test_size=0.20)\n",
    "\n",
    "df_validation = pd.concat([X_test, y_test], axis=1)[['id', 'label', 'text']].reset_index(drop=True)\n",
    "df_train = pd.concat([X_train, y_train], axis=1)[['id', 'label', 'text']].reset_index(drop=True)\n",
    "\n",
    "print(df_train.label.value_counts(normalize=True))\n",
    "print(df_validation.label.value_counts(normalize=True))\n",
    "\n",
    "# df_train.to_csv('train.tsv', encoding='utf-8', index=False, sep='\\t')\n",
    "# df_validation.to_csv('test.tsv', encoding='utf-8', index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert train data and test data to a format where model expects\n",
    "records = pd.get_dummies(df_train[['label', 'text']], columns=['label'], dtype='float').to_dict(orient='records')\n",
    "TRAIN_DATA = []\n",
    "for record in records:\n",
    "    cats = record.copy()\n",
    "    cats.pop('text')\n",
    "    train_record = (record.get('text'), {\"cats\": cats})\n",
    "    TRAIN_DATA.append(train_record)\n",
    "    \n",
    "records = pd.get_dummies(df_validation[['label', 'text']], columns=['label'], dtype='float').to_dict(orient='records')\n",
    "EVAL_DATA = []\n",
    "for record in records:\n",
    "    cats = record.copy()\n",
    "    cats.pop('text')\n",
    "    eval_record = (record.get('text'), {\"cats\": cats})\n",
    "    EVAL_DATA.append(eval_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training sentiment classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import json\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import spacy\n",
    "import torch\n",
    "from spacy.util import minibatch\n",
    "import tqdm\n",
    "import unicodedata\n",
    "import wasabi\n",
    "from spacy_transformers.util import cyclic_triangular_rate\n",
    "import en_trf_bertbaseuncased_lg\n",
    "\n",
    "\n",
    "def read_inputs(train_data):\n",
    "    texts = []\n",
    "    cats = []\n",
    "    for line in train_data:\n",
    "        text, gold = line\n",
    "        text = preprocess_text(text)\n",
    "        texts.append(text)\n",
    "        cats.append(gold[\"cats\"])\n",
    "    return texts, cats\n",
    "\n",
    "\n",
    "def make_sentence_examples(nlp, texts, labels):\n",
    "    \"\"\"Treat each sentence of the document as an instance, using the doc labels.\"\"\"\n",
    "    sents = []\n",
    "    sent_cats = []\n",
    "    for text, cats in zip(texts, labels):\n",
    "        doc = nlp.make_doc(text)\n",
    "        doc = nlp.get_pipe(\"sentencizer\")(doc)\n",
    "        for sent in doc.sents:\n",
    "            sents.append(sent.text)\n",
    "            sent_cats.append(cats)\n",
    "    return sents, sent_cats\n",
    "\n",
    "\n",
    "white_re = re.compile(r\"\\s\\s+\")\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.replace(\"<s>\", \"<open-s-tag>\")\n",
    "    text = text.replace(\"</s>\", \"<close-s-tag>\")\n",
    "    text = white_re.sub(\" \", text).strip()\n",
    "    return \"\".join(\n",
    "        c for c in unicodedata.normalize(\"NFD\", text) if unicodedata.category(c) != \"Mn\"\n",
    "    )\n",
    "\n",
    "\n",
    "def evaluate(nlp, texts, cats, pos_label):\n",
    "    tp = 0.0  # True positives\n",
    "    fp = 0.0  # False positives\n",
    "    fn = 0.0  # False negatives\n",
    "    tn = 0.0  # True negatives\n",
    "    total_words = sum(len(text.split()) for text in texts)\n",
    "    with tqdm.tqdm(total=total_words, leave=False) as pbar:\n",
    "        for i, doc in enumerate(nlp.pipe(texts, batch_size=8)):\n",
    "            gold = cats[i]\n",
    "            for label, score in doc.cats.items():\n",
    "                if label not in gold:\n",
    "                    continue\n",
    "                if label != pos_label:\n",
    "                    continue\n",
    "                if score >= 0.5 and gold[label] >= 0.5:\n",
    "                    tp += 1.0\n",
    "                elif score >= 0.5 and gold[label] < 0.5:\n",
    "                    fp += 1.0\n",
    "                elif score < 0.5 and gold[label] < 0.5:\n",
    "                    tn += 1\n",
    "                elif score < 0.5 and gold[label] >= 0.5:\n",
    "                    fn += 1\n",
    "            pbar.update(len(doc.text.split()))\n",
    "    precision = tp / (tp + fp + 1e-8)\n",
    "    recall = tp / (tp + fn + 1e-8)\n",
    "    if (precision + recall) == 0:\n",
    "        f_score = 0.0\n",
    "    else:\n",
    "        f_score = 2 * (precision * recall) / (precision + recall)\n",
    "    return {\"textcat_p\": precision, \"textcat_r\": recall, \"textcat_f\": f_score}\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model=\"en_trf_bertbaseuncased_lg\",\n",
    "    train_data=None,\n",
    "    eval_data=None,\n",
    "    output_dir=None,\n",
    "    n_iter=5,\n",
    "    n_texts=100,\n",
    "    batch_size=8,\n",
    "    learn_rate=2e-5,\n",
    "    max_wpb=1000,\n",
    "    use_test=False,\n",
    "    pos_label=None,\n",
    "):\n",
    "    spacy.util.fix_random_seed(0)\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "\n",
    "    try:\n",
    "        nlp = spacy.load(model)\n",
    "    except:\n",
    "        nlp = en_trf_bertbaseuncased_lg.load()\n",
    "        \n",
    "    print(nlp.pipe_names)\n",
    "    print(f\"Loaded model '{model}'\")\n",
    "    textcat = nlp.create_pipe(\n",
    "        \"trf_textcat\",\n",
    "        config={\"architecture\": \"softmax_last_hidden\", \"words_per_batch\": max_wpb},\n",
    "    )\n",
    "    \n",
    "    train_texts, train_cats = read_inputs(train_data)\n",
    "    eval_texts, eval_cats = read_inputs(eval_data)\n",
    "    labels = set()\n",
    "    for cats in train_cats + eval_cats:\n",
    "        labels.update(cats)\n",
    "    # use the first label in the set as the positive label if one isn't\n",
    "    # provided\n",
    "    for label in sorted(labels):\n",
    "        if not pos_label:\n",
    "            pos_label = label\n",
    "        textcat.add_label(label)\n",
    "\n",
    "\n",
    "    print(\"Labels:\", textcat.labels)\n",
    "    print(\"Positive label for evaluation:\", pos_label)\n",
    "    nlp.add_pipe(textcat, last=True)\n",
    "    print(f\"Using {len(train_texts)} training docs, {len(eval_texts)} evaluation\")\n",
    "    split_training_by_sentence = False\n",
    "    if split_training_by_sentence:\n",
    "        # If we're using a model that averages over sentence predictions (we are),\n",
    "        # there are some advantages to just labelling each sentence as an example.\n",
    "        # It means we can mix the sentences into different batches, so we can make\n",
    "        # more frequent updates. It also changes the loss somewhat, in a way that's\n",
    "        # not obviously better -- but it does seem to work well.\n",
    "        train_texts, train_cats = make_sentence_examples(nlp, train_texts, train_cats)\n",
    "        print(f\"Extracted {len(train_texts)} training sents\")\n",
    "    # total_words = sum(len(text.split()) for text in train_texts)\n",
    "    train_data = list(zip(train_texts, [{\"cats\": cats} for cats in train_cats]))\n",
    "    # Initialize the TextCategorizer, and create an optimizer.\n",
    "    optimizer = nlp.resume_training()\n",
    "    optimizer.alpha = 0.001\n",
    "    optimizer.trf_weight_decay = 0.005\n",
    "    optimizer.L2 = 0.0\n",
    "    learn_rates = cyclic_triangular_rate(\n",
    "        learn_rate / 3, learn_rate * 3, 2 * len(train_data) // batch_size\n",
    "    )\n",
    "    print(\"Training the model...\")\n",
    "    print(\"{:^5}\\t{:^5}\\t{:^5}\\t{:^5}\".format(\"LOSS\", \"P\", \"R\", \"F\"))\n",
    "\n",
    "    pbar = tqdm.tqdm(total=100, leave=False)\n",
    "    results = []\n",
    "    epoch = 0\n",
    "    step = 0\n",
    "    eval_every = 100\n",
    "    patience = 3\n",
    "    while True:\n",
    "        # Train and evaluate\n",
    "        losses = Counter()\n",
    "        random.shuffle(train_data)\n",
    "        batches = minibatch(train_data, size=batch_size)\n",
    "        for batch in batches:\n",
    "            optimizer.trf_lr = next(learn_rates)\n",
    "            texts, annotations = zip(*batch)\n",
    "            nlp.update(texts, annotations, sgd=optimizer, drop=0.1, losses=losses)\n",
    "            pbar.update(1)\n",
    "            if step and (step % eval_every) == 0:\n",
    "                pbar.close()\n",
    "                with nlp.use_params(optimizer.averages):\n",
    "                    scores = evaluate(nlp, eval_texts, eval_cats, pos_label)\n",
    "                results.append((scores[\"textcat_f\"], step, epoch))\n",
    "                print(\n",
    "                    \"{0:.3f}\\t{1:.3f}\\t{2:.3f}\\t{3:.3f}\".format(\n",
    "                        losses[\"trf_textcat\"],\n",
    "                        scores[\"textcat_p\"],\n",
    "                        scores[\"textcat_r\"],\n",
    "                        scores[\"textcat_f\"],\n",
    "                    )\n",
    "                )\n",
    "                pbar = tqdm.tqdm(total=eval_every, leave=False)\n",
    "            step += 1\n",
    "        epoch += 1\n",
    "        # Stop if no improvement in HP.patience checkpoints\n",
    "        if results:\n",
    "            best_score, best_step, best_epoch = max(results)\n",
    "            if ((step - best_step) // eval_every) >= patience:\n",
    "                break\n",
    "\n",
    "    msg = wasabi.Printer()\n",
    "    table_widths = [2, 4, 6]\n",
    "    msg.info(f\"Best scoring checkpoints\")\n",
    "    msg.row([\"Epoch\", \"Step\", \"Score\"], widths=table_widths)\n",
    "    msg.row([\"-\" * width for width in table_widths])\n",
    "    for score, step, epoch in sorted(results, reverse=True)[:10]:\n",
    "        msg.row([epoch, step, \"%.2f\" % (score * 100)], widths=table_widths)\n",
    "\n",
    "    # Test the trained model\n",
    "    test_text = eval_texts[0]\n",
    "    doc = nlp(test_text)\n",
    "    print(test_text, doc.cats)\n",
    "\n",
    "    if output_dir is not None:\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)\n",
    "        # test the saved model\n",
    "        print(\"Loading from\", output_dir)\n",
    "        nlp2 = spacy.load(output_dir)\n",
    "        doc2 = nlp2(test_text)\n",
    "        print(test_text, doc2.cats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sentencizer', 'trf_wordpiecer', 'trf_tok2vec']\n",
      "Loaded model 'en_trf_bertbaseuncased_lg'\n",
      "Labels: ('label_neg', 'label_neu', 'label_pos')\n",
      "Positive label for evaluation: label_pos\n",
      "Using 1086 training docs, 272 evaluation\n",
      "Training the model...\n",
      "LOSS \t  P  \t  R  \t  F  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.726\t0.771\t0.831\t0.800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.177\t0.831\t0.831\t0.831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.051\t0.775\t0.846\t0.809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.221\t0.818\t0.831\t0.824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.019\t0.812\t0.862\t0.836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 15/100 [01:10<06:26,  4.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Best scoring checkpoints\u001b[0m\n",
      "Epoch   Step   Score \n",
      "--   ----   ------\n",
      "3    500    84.62 \n",
      "4    600    83.58 \n",
      "1    200    83.08 \n",
      "5    700    82.71 \n",
      "2    400    82.44 \n",
      "5    800    82.09 \n",
      "2    300    80.88 \n",
      "0    100    80.00 \n",
      "tributes paid to the muchloved amp compassionate bury gp dr saad aldubbaisi who has died with coronavirus the iraqiborn gp graduated from the university of baghdad amp had worked in the bury area for 20 years rest in peace {'label_neg': 2.5065755835385062e-05, 'label_neu': 3.924364864360541e-05, 'label_pos': 0.9999356269836426}\n",
      "Saved model to /home/jupyter/projects/sentiment-classifier\n",
      "Loading from /home/jupyter/projects/sentiment-classifier\n",
      "tributes paid to the muchloved amp compassionate bury gp dr saad aldubbaisi who has died with coronavirus the iraqiborn gp graduated from the university of baghdad amp had worked in the bury area for 20 years rest in peace {'label_neg': 2.5065755835385062e-05, 'label_neu': 3.924364864360541e-05, 'label_pos': 0.9999356269836426}\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"model\": \"en_trf_bertbaseuncased_lg\",\n",
    "    \"train_data\": TRAIN_DATA,\n",
    "    \"eval_data\": EVAL_DATA,\n",
    "    \"output_dir\": \"models/sentiment-classifier\",\n",
    "    \"use_test\": False,\n",
    "    \"batch_size\": 8,\n",
    "    \"learn_rate\": 2e-5,\n",
    "    \"max_wpb\": 1000,\n",
    "    \"n_texts\": 100,\n",
    "    \"n_iter\": 5,\n",
    "    \"pos_label\": \"label_pos\",\n",
    "}\n",
    "\n",
    "\n",
    "train_model(**params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import unicodedata\n",
    "\n",
    "# Review model output\n",
    "\n",
    "def read_inputs(train_data):\n",
    "    texts = []\n",
    "    cats = []\n",
    "    for line in train_data:\n",
    "        text, gold = line\n",
    "        text = preprocess_text(text)\n",
    "        texts.append(text)\n",
    "        cats.append(gold[\"cats\"])\n",
    "    return texts, cats\n",
    "\n",
    "\n",
    "white_re = re.compile(r\"\\s\\s+\")\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.replace(\"<s>\", \"<open-s-tag>\")\n",
    "    text = text.replace(\"</s>\", \"<close-s-tag>\")\n",
    "    text = white_re.sub(\" \", text).strip()\n",
    "    return \"\".join(\n",
    "        c for c in unicodedata.normalize(\"NFD\", text) if unicodedata.category(c) != \"Mn\"\n",
    "    )\n",
    "\n",
    "output_dir =  \"models/sentiment-classifier\"\n",
    "eval_texts, eval_cats = read_inputs(EVAL_DATA)\n",
    "i = 0 # Choose a random number to test\n",
    "test_text = eval_texts[i] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how can a city go bankrupt as liverpool council reveal £44m shortfall ten years of vindictive savage tory cuts and robert jenrick reneging on deal to fund coronavirus losses would do that via {'label_neg': 0.9998134970664978, 'label_neu': 0.00014070399629417807, 'label_pos': 4.5758461055811495e-05}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(output_dir)\n",
    "doc = nlp(test_text)\n",
    "print(test_text, doc.cats)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are some good news coming as coronavirus vaccine research advances. {'label_neg': 4.7637462557759136e-05, 'label_neu': 0.00017945301078725606, 'label_pos': 0.9997729659080505}\n"
     ]
    }
   ],
   "source": [
    "# Test the trained model with text of your choice\n",
    "test_text = \"There are some good news coming as coronavirus vaccine research advances.\"\n",
    "doc = nlp(test_text)\n",
    "print(test_text, doc.cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are some bad news coming as coronavirus vaccine research fails. {'label_neg': 0.9993046522140503, 'label_neu': 0.0006404068553820252, 'label_pos': 5.4913893109187484e-05}\n"
     ]
    }
   ],
   "source": [
    "# Test the trained model with text of your choice\n",
    "test_text = \"There are some bad news coming as coronavirus vaccine research fails.\"\n",
    "doc = nlp(test_text)\n",
    "print(test_text, doc.cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extend tweets with extended tweets and original retweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22250\n",
      "39873\n",
      "Tweet text saved to file!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Read JSON file that contains tweet data\n",
    "with open('data/tweets/data.json', 'r',) as f:\n",
    "    tweets = json.load(f)\n",
    "    \n",
    "# Check whether all tweets are unique tweets\n",
    "unique_tweets = []\n",
    "ids = []\n",
    "for t in tweets:\n",
    "    if not t.get('id') in ids:\n",
    "        ids.append(t.get('id'))\n",
    "        unique_tweets.append(t)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "len(unique_tweets)\n",
    "\n",
    "# Check number of tweets\n",
    "print(len(tweets))\n",
    "\n",
    "# Extract text content from individual tweets\n",
    "write_text_tweets = True\n",
    "\n",
    "if write_text_tweets:\n",
    "    \n",
    "    text_tweets = []\n",
    "    for t in unique_tweets:\n",
    "        \n",
    "        t_ = {}\n",
    "        t_['id'] = t.get('id')\n",
    "        t_['created_at'] = t.get('created_at')\n",
    "        if t.get('extended_tweet'):\n",
    "            t_['text'] = t.get('extended_tweet').get('full_text')\n",
    "        else:\n",
    "            t_['text'] = t.get('text')\n",
    "            \n",
    "        text_tweets.append(t_)\n",
    "            \n",
    "        t_r = {}\n",
    "        if t.get('retweeted_status'):\n",
    "            t_r['id'] = t.get('retweeted_status').get('id')\n",
    "            t_r['created_at'] = t.get('retweeted_status').get('created_at')\n",
    "            if t.get('retweeted_status').get('extended_tweet'):\n",
    "                t_r['text'] = t.get('retweeted_status').get('extended_tweet').get('full_text')\n",
    "            else:\n",
    "                t_r['text'] = t.get('retweeted_status').get('text')\n",
    "              \n",
    "            text_tweets.append(t_r)\n",
    "            \n",
    "        else:\n",
    "            pass\n",
    "\n",
    "\n",
    "\n",
    "    print(len(text_tweets))\n",
    "\n",
    "    df = pd.DataFrame(text_tweets)\n",
    "    df.to_csv('data/tweets/tweets_extended.csv', encoding='utf-8', index=False)\n",
    "    print('Tweet text saved to file!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39873, 3)\n",
      "(31009, 3)\n",
      "31009\n",
      "30992\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from helper_functions import preprocess, remove_emojis\n",
    "import re\n",
    "import unicodedata\n",
    "import emoji\n",
    "\n",
    "df = pd.read_csv('data/tweets/tweets_extended.csv', encoding='utf-8', )\n",
    "print(df.shape)\n",
    "# df = df[['id', 'label', 'text']]\n",
    "# df = df[df['label'].isnull()]\n",
    "df.drop_duplicates(subset=['id'], inplace=True)\n",
    "df['text'] = df['text'].astype(str)\n",
    "df['text'].replace('', np.nan, inplace=True)\n",
    "df['text'].replace(' ', np.nan, inplace=True)\n",
    "df.dropna(subset=['text'], inplace=True)\n",
    "df['text'] = df['text'].astype(str)\n",
    "print(df.shape)\n",
    "df['text'] = df['text'].replace(r'\\n',' ',regex=True)\n",
    "df['text'] = df['text'].replace('“',' ',regex=True)\n",
    "df['text'] = df['text'].apply(preprocess)\n",
    "df['text'] = df['text'].str.lower()\n",
    "\n",
    "ids = df['id'].values.tolist()\n",
    "texts = df['text'].values.tolist()\n",
    "\n",
    "white_re = re.compile(r\"\\s\\s+\")\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.replace(\"<s>\", \"<open-s-tag>\")\n",
    "    text = text.replace(\"</s>\", \"<close-s-tag>\")\n",
    "    text = white_re.sub(\" \", text).strip()\n",
    "    text = emoji.demojize(text)\n",
    "    return \"\".join(\n",
    "        c for c in unicodedata.normalize(\"NFD\", text) if unicodedata.category(c) != \"Mn\"\n",
    "    )\n",
    "\n",
    "\n",
    "texts = [str(preprocess_text(text)) for text in texts]\n",
    "print(len(texts))\n",
    "\n",
    "texts = [x for x in texts if x not in ['rt', '']] # add this to avoid AssertionError\n",
    "print(len(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "output_dir =  \"models/sentiment-classifier\"\n",
    "nlp = spacy.load(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcf662d2193b42609f34dd45f6537963",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batches', max=10.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09d2c874c56c456a8a4491587831f7a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='docs', max=3101.0, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3387a420ec44a5ea8130db80b2d39ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='docs', max=3101.0, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8a1974079374e168c42e0b8e6e757e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='docs', max=3101.0, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a8d8d3cb3374b4388baf6715411e1ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='docs', max=3101.0, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1552992f88274598ae087b6f76c8aff5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='docs', max=3101.0, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98e9ee2111504e62bb2ea12ff41708d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='docs', max=3101.0, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cbabb2028454147982e78ce2a2e85fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='docs', max=3101.0, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0bfb5d3bd5a4970809cda935b0d2b35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='docs', max=3101.0, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74487db98ebd4abb87ec96d10abea409",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='docs', max=3101.0, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa65f679ea2f4014a295d50b532d7949",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='docs', max=3100.0, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 1245501194375700483,\n",
       "  'text': 'rt why is germany able to test for coronavirus so much more than the uk',\n",
       "  'label_neg': 6.44201209070161e-05,\n",
       "  'label_neu': 0.00010541563824517652,\n",
       "  'label_pos': 0.9998301267623901},\n",
       " {'id': 1245075899558625280,\n",
       "  'text': 'why is germany able to test for coronavirus so much more than the uk',\n",
       "  'label_neg': 7.429449033224955e-05,\n",
       "  'label_neu': 0.000109510263428092,\n",
       "  'label_pos': 0.9998162388801575},\n",
       " {'id': 1245501189208276996,\n",
       "  'text': 'rt absolutely wrong how uk s coronavirus test strategy unravelled coronavirus outbreak the guardian',\n",
       "  'label_neg': 0.9991256594657898,\n",
       "  'label_neu': 0.000657263386528939,\n",
       "  'label_pos': 0.0002170733641833067}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_ = []\n",
    "\n",
    "import time\n",
    "from tqdm import trange\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "arr_texts = np.array_split(np.array(texts), 10)\n",
    "arr_ids = np.array_split(np.array(ids), 10)\n",
    "\n",
    "\n",
    "for batch_no, batch_ids, batch_texts in tqdm(zip(range(len(arr_ids)), arr_ids, arr_texts), total=len(arr_ids), desc='batches'):\n",
    "    try:\n",
    "        texts_ = batch_texts.tolist()\n",
    "        ids_ = batch_ids.tolist()\n",
    "\n",
    "        for i_, doc_ in tqdm(zip(ids_, nlp.pipe(texts_)), desc='docs', total=len(ids_), leave=False):\n",
    "            try:\n",
    "                prediction_ = {\n",
    "                    'id': i_,\n",
    "                    'text': doc_.text,\n",
    "                }\n",
    "                prediction_.update(doc_.cats)\n",
    "                predictions_.append(prediction_)\n",
    "            except:\n",
    "                print(f'Twit with {i_} is skipped due to an error.')\n",
    "                pass\n",
    "\n",
    "            time.sleep(0.01)\n",
    "\n",
    "    except:\n",
    "        print(f'Batch {batch_no} is skipped due to an error.')\n",
    "        pass\n",
    "            \n",
    "\n",
    "predictions_[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30992"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30992, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>label_neg</th>\n",
       "      <th>label_neu</th>\n",
       "      <th>label_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1245501194375700483</td>\n",
       "      <td>rt why is germany able to test for coronavirus...</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.999830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1245075899558625280</td>\n",
       "      <td>why is germany able to test for coronavirus so...</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.999816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1245501189208276996</td>\n",
       "      <td>rt absolutely wrong how uk s coronavirus test ...</td>\n",
       "      <td>0.999126</td>\n",
       "      <td>0.000657</td>\n",
       "      <td>0.000217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1245498501011775489</td>\n",
       "      <td>absolutely wrong how uk s coronavirus test str...</td>\n",
       "      <td>0.999575</td>\n",
       "      <td>0.000324</td>\n",
       "      <td>0.000101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1245501189136867328</td>\n",
       "      <td>rt us coast guard orders foreign cruise ships ...</td>\n",
       "      <td>0.000847</td>\n",
       "      <td>0.997944</td>\n",
       "      <td>0.001209</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                               text  \\\n",
       "0  1245501194375700483  rt why is germany able to test for coronavirus...   \n",
       "1  1245075899558625280  why is germany able to test for coronavirus so...   \n",
       "2  1245501189208276996  rt absolutely wrong how uk s coronavirus test ...   \n",
       "3  1245498501011775489  absolutely wrong how uk s coronavirus test str...   \n",
       "4  1245501189136867328  rt us coast guard orders foreign cruise ships ...   \n",
       "\n",
       "   label_neg  label_neu  label_pos  \n",
       "0   0.000064   0.000105   0.999830  \n",
       "1   0.000074   0.000110   0.999816  \n",
       "2   0.999126   0.000657   0.000217  \n",
       "3   0.999575   0.000324   0.000101  \n",
       "4   0.000847   0.997944   0.001209  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = pd.DataFrame(predictions_)\n",
    "output.to_csv('data/tweets/output_extended.csv', encoding='utf-8', index=False)\n",
    "print(output.shape)\n",
    "output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Copy of Finetune COVID-Twitter-BERT",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "name": "common-cu101.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu101:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
