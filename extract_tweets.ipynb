{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter data extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "\n",
    "# Load Twitter API client\n",
    "from searchtweets import ResultStream, gen_rule_payload, load_credentials, collect_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define API arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grabbing bearer token from OAUTH\n",
      "Grabbing bearer token from OAUTH\n"
     ]
    }
   ],
   "source": [
    "# Define arguments to be able to connect to Twitter API for the last 30 days of tweets\n",
    "premium_30day_search_args = load_credentials(filename=\".twitter_keys.yaml\",\n",
    "                 yaml_key=\"search_30day_tweets_api\",\n",
    "                 env_overwrite=False)\n",
    "\n",
    "# Define arguments to be able to connect to Twitter API for the last 30 days of tweets\n",
    "premium_fullarchive_search_args = load_credentials(filename=\".twitter_keys.yaml\",\n",
    "                 yaml_key=\"search_fullarchive_tweets_api\",\n",
    "                 env_overwrite=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract last 30 days data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract last 30 days tweets using Twitter API\n",
    "# This is due to tweets older than 30 days use a different end point and limit quota\n",
    "\n",
    "write_to_disk = False\n",
    "if write_to_disk:\n",
    "    all_tweets = []\n",
    "    for t in list(range(1,30)):\n",
    "        from_date = str(t).zfill(2)\n",
    "        to_date = str(t+1).zfill(2)\n",
    "\n",
    "        rule = gen_rule_payload(\n",
    "            \"UK coronavirus lang:en\",\n",
    "            from_date=f\"2020-05-{from_date}\", \n",
    "            to_date=f\"2020-05-{to_date}\",\n",
    "            results_per_call=100\n",
    "        )\n",
    "\n",
    "        tweets = collect_results(rule, max_results=500, result_stream_args=premium_30day_search_args) \n",
    "\n",
    "        all_tweets.extend(tweets)\n",
    "        print(len(all_tweets))\n",
    "        # Set a sleep time to avoid hitting API limits\n",
    "        time.sleep(10)\n",
    "\n",
    "        # Write extacted tweets to a JSON file\n",
    "        with open('data.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(all_tweets, f, indent=4, ensure_ascii=False)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract data older than 30 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract tweets older than 30 days\n",
    "# This is due to tweets older than 30 days use a different end point and limit quota\n",
    "\n",
    "# Extract tweets older then 30 days using Twitter API\n",
    "write_to_disk = False\n",
    "if write_to_disk:\n",
    "    month = '04' # specify month number\n",
    "#     all_tweets = []\n",
    "    for t in list(range(1,30)):\n",
    "        from_date = str(t).zfill(2)\n",
    "        to_date = str(t+1).zfill(2)\n",
    "\n",
    "        rule = gen_rule_payload(\n",
    "            \"UK coronavirus lang:en\",\n",
    "            from_date=f\"2020-{month}-{from_date}\", \n",
    "            to_date=f\"2020-{month}-{to_date}\",\n",
    "            results_per_call=100\n",
    "        )\n",
    "\n",
    "        tweets = collect_results(rule, max_results=150, result_stream_args=premium_fullarchive_search_args) \n",
    "\n",
    "        all_tweets.extend(tweets)\n",
    "        print(f\"2020-{month}-{to_date}\")\n",
    "        print(len(all_tweets))\n",
    "        print('---')\n",
    "        # Set a sleep time to avoid hitting API limits\n",
    "        time.sleep(10)\n",
    "\n",
    "        # Write extacted tweets to a JSON file\n",
    "        with open('data.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(all_tweets, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load tweet data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22250"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read JSON file that contains tweet data\n",
    "with open('data.json', 'r',) as f:\n",
    "    tweets = json.load(f)\n",
    "\n",
    "# Check number of tweets\n",
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @reactionlife: Why is Germany able to test for coronavirus so much more than the UK? - @reactionlife https://t.co/9xltrMYhNu \n",
      "\n",
      "\n",
      "RT @cdhawesi: 'Absolutely wrong': how UK's coronavirus test strategy unravelled | Coronavirus outbreak | The Guardian https://t.co/gJK3mppm… \n",
      "\n",
      "\n",
      "RT @Independent: US Coast Guard orders foreign cruise ships to care for suspected coronavirus passengers on board 'indefinitely' https://t.… \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print twitter text sample\n",
    "[print(tweet.get('text'),'\\n\\n') for tweet in tweets[0:3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['contributors',\n",
       " 'coordinates',\n",
       " 'created_at',\n",
       " 'display_text_range',\n",
       " 'entities',\n",
       " 'extended_entities',\n",
       " 'extended_tweet',\n",
       " 'favorite_count',\n",
       " 'favorited',\n",
       " 'filter_level',\n",
       " 'geo',\n",
       " 'id',\n",
       " 'id_str',\n",
       " 'in_reply_to_screen_name',\n",
       " 'in_reply_to_status_id',\n",
       " 'in_reply_to_status_id_str',\n",
       " 'in_reply_to_user_id',\n",
       " 'in_reply_to_user_id_str',\n",
       " 'is_quote_status',\n",
       " 'lang',\n",
       " 'matching_rules',\n",
       " 'place',\n",
       " 'possibly_sensitive',\n",
       " 'quote_count',\n",
       " 'quoted_status',\n",
       " 'quoted_status_id',\n",
       " 'quoted_status_id_str',\n",
       " 'quoted_status_permalink',\n",
       " 'reply_count',\n",
       " 'retweet_count',\n",
       " 'retweeted',\n",
       " 'retweeted_status',\n",
       " 'source',\n",
       " 'text',\n",
       " 'truncated',\n",
       " 'user']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check all keys associated with individual tweet objects\n",
    "tweet_keys = []\n",
    "for t in tweets:\n",
    "    tweet_keys.extend(list(t.keys()))\n",
    "    tweet_keys = list(set(tweet_keys))\n",
    "    \n",
    "tweet_keys = sorted(tweet_keys)\n",
    "tweet_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22250"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check whether all tweets are unique tweets\n",
    "unique_tweets = []\n",
    "ids = []\n",
    "for t in tweets:\n",
    "    if not t.get('id') in ids:\n",
    "        ids.append(t.get('id'))\n",
    "        unique_tweets.append(t)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "len(unique_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22250\n",
      "Tweet text saved to file!\n"
     ]
    }
   ],
   "source": [
    "# Extract text content from individual tweets\n",
    "write_text_tweets = False\n",
    "\n",
    "if write_text_tweets:\n",
    "    \n",
    "    text_tweets = []\n",
    "    for t in unique_tweets:\n",
    "        t_ = {}\n",
    "        if t.get('retweeted_status'):\n",
    "            t_['id'] = t.get('retweeted_status').get('id')\n",
    "            t_['created_at'] = t.get('retweeted_status').get('created_at')\n",
    "            if t.get('retweeted_status').get('extended_tweet'):\n",
    "                t_['text'] = t.get('retweeted_status').get('extended_tweet').get('full_text')\n",
    "            else:\n",
    "                t_['text'] = t.get('retweeted_status').get('text')\n",
    "        else:\n",
    "            t_['id'] = t.get('id')\n",
    "            t_['created_at'] = t.get('created_at')\n",
    "            if t.get('extended_tweet'):\n",
    "                t_['text'] = t.get('extended_tweet').get('full_text')\n",
    "            else:\n",
    "                t_['text'] = t.get('text')\n",
    "\n",
    "        text_tweets.append(t_)\n",
    "\n",
    "    print(len(text_tweets))\n",
    "\n",
    "    df = pd.DataFrame(text_tweets)\n",
    "    df.to_csv('tweets.csv', encoding='utf-8', index=False)\n",
    "    print('Tweet text saved to file!')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emojis(text):\n",
    "    import re\n",
    "    emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "    return re.sub(emoj, '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define text processing function\n",
    "def preprocess(text):\n",
    "    import re, string\n",
    "    \n",
    "    text = text.replace('(<br/>)', ' ')\n",
    "    text = text.replace('(&amp)', ' ')\n",
    "    text = text.replace('(&gt)', ' ')\n",
    "    text = text.replace('(&lt)', ' ')\n",
    "    text = text.replace('(\\xa0)', ' ')\n",
    "    text = text.replace(r'\\n',' ',)\n",
    "    text = text.replace('\"',' ',)\n",
    "    text = text.replace(\"'\",' ',)\n",
    "    \n",
    "    # Remove URL\n",
    "    text = re.sub(r\"(?:\\@|http?\\://|https?\\://|www.)\\S+\", \"\", text)\n",
    "    \n",
    "    # Remove emoji\n",
    "    text = remove_emojis(text)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans(' ', ' ', string.punctuation))\n",
    "    \n",
    "    # Remove white space\n",
    "    text = \" \".join(text.split())\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing labeled data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "labeled = pd.read_csv('tweets_labeled.csv', encoding='utf-8')\n",
    "labeled = labeled[labeled['label'].notnull()]\n",
    "labeled.rename(columns={'text':'sentence'}, inplace=True)\n",
    "labeled['sentence'] = labeled['sentence'].replace(r'\\n',' ',regex=True)\n",
    "labeled['sentence'] = labeled['sentence'].apply(preprocess)\n",
    "labeled['sentence'] = labeled['sentence'].str.lower()\n",
    "labeled['label'] = labeled['label'].astype(int)\n",
    "labeled['label'] = labeled['label'].astype(str)\n",
    "print(labeled.shape)\n",
    "labeled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled.label.value_counts(normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = labeled.copy(deep=True)\n",
    "\n",
    "X = df[['sentence']]\n",
    "y = df[['label']]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                stratify=y, \n",
    "                                                test_size=0.20)\n",
    "\n",
    "df_test = pd.concat([X_test, y_test], axis=1)\n",
    "df_train = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "df_validation = df_train.groupby('label').apply(lambda x: x.sample(frac=0.25)).reset_index(drop=True)\n",
    "\n",
    "print(df_train.label.value_counts(normalize=True))\n",
    "print(df_validation.label.value_counts(normalize=True))\n",
    "print(df_test.label.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save train, validation and test set for training based on Tensorflow BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('train.tsv', encoding='utf-8', index=False, sep='\\t')\n",
    "df_validation.to_csv('dev.tsv', encoding='utf-8', index=False, sep='\\t')\n",
    "df_test.to_csv('test.tsv', encoding='utf-8', index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis with custom trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis with vaderSentiment module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "analyser = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sentiment_scores(sentence):\n",
    "    snt = analyser.polarity_scores(sentence)\n",
    "    print(\"{:-<40} {}\".format(sentence, str(snt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_sentiment_scores(labeled.text.values[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment 140 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1048576, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>date_of_tweet</th>\n",
       "      <th>query</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811372</td>\n",
       "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811592</td>\n",
       "      <td>Mon Apr 06 22:20:03 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mybirch</td>\n",
       "      <td>Need a hug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811594</td>\n",
       "      <td>Mon Apr 06 22:20:03 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>coZZ</td>\n",
       "      <td>@LOLTrish hey  long time no see! Yes.. Rains a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811795</td>\n",
       "      <td>Mon Apr 06 22:20:05 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>2Hood4Hollywood</td>\n",
       "      <td>@Tatiana_K nope they didn't have it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>1467812025</td>\n",
       "      <td>Mon Apr 06 22:20:09 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mimismo</td>\n",
       "      <td>@twittera que me muera ?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   polarity    tweet_id                 date_of_tweet     query  \\\n",
       "0         0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1         0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2         0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3         0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4         0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "5         0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY   \n",
       "6         0  1467811592  Mon Apr 06 22:20:03 PDT 2009  NO_QUERY   \n",
       "7         0  1467811594  Mon Apr 06 22:20:03 PDT 2009  NO_QUERY   \n",
       "8         0  1467811795  Mon Apr 06 22:20:05 PDT 2009  NO_QUERY   \n",
       "9         0  1467812025  Mon Apr 06 22:20:09 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  \n",
       "5         joy_wolf                      @Kwesidei not the whole crew   \n",
       "6          mybirch                                        Need a hug   \n",
       "7             coZZ  @LOLTrish hey  long time no see! Yes.. Rains a...  \n",
       "8  2Hood4Hollywood               @Tatiana_K nope they didn't have it   \n",
       "9          mimismo                          @twittera que me muera ?   "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\n",
    "    'sentiment_140/training_utf.csv',\n",
    "    encoding='utf-8',\n",
    "    names=[\n",
    "        'polarity',\n",
    "        'tweet_id',\n",
    "        'date_of_tweet',\n",
    "        'query',\n",
    "        'user',\n",
    "        'text',\n",
    "    ]\n",
    ")\n",
    "print(df.shape)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
